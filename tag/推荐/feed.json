{
    "version": "https://jsonfeed.org/version/1",
    "title": "Durtime • All posts by \"推荐\" tag",
    "description": "The aroma of black tea no longer exists",
    "home_page_url": "https://durtime.github.io/blog",
    "items": [
        {
            "id": "https://durtime.github.io/blog/posts/1fd8e8b9/",
            "url": "https://durtime.github.io/blog/posts/1fd8e8b9/",
            "title": "推荐业务(一)",
            "date_published": "2023-03-12T01:34:41.000Z",
            "content_html": "<h1 id=\"1-离线画像业务\"><a class=\"markdownIt-Anchor\" href=\"#1-离线画像业务\">#</a> 1 离线画像业务</h1>\n<p>画像的构建作为推荐系统非常重要的环节，画像可以作为整个产品的推荐或者营销重要依据。需要通过各种方法来构建。</p>\n<p>文章内容标签化：内容标签化，根据内容定性的制定一系列标签，这些标签可以是描述性标签。针对于文章就是文章相关的内容词语。</p>\n<p>文章的关键词、主题词</p>\n<h1 id=\"2-textrank算法\"><a class=\"markdownIt-Anchor\" href=\"#2-textrank算法\">#</a> 2 TextRank 算法</h1>\n<p>TextRank 算法是一种基于图的用于关键词抽取和文档摘要的排序算法，由谷歌的网页重要性排序算法 PageRank 算法改进而来，它利用一篇文档内部的词语间的共现信息 (语义) 便可以抽取关键词，它能够从一个给定的文本中抽取出该文本的关键词、关键词组，并使用抽取式的自动文摘方法抽取出该文本的关键句。</p>\n<p>TextRank 算法的基本思想是将文档看作一个词的网络，该网络中的链接表示词与词之间的语义关系。</p>\n<p>TextRank 算法主要包括：关键词抽取、关键短语抽取、关键句抽取。</p>\n<p>（1）关键词抽取（keyword extraction）<br>\n关键词抽取是指从文本中确定一些能够描述文档含义的术语的过程。对关键词抽取而言，用于构建顶点集的文本单元可以是句子中的一个或多个字；根据这些字之间的关系（比如：在一个框中同时出现）构建边。根据任务的需要，可以使用语法过滤器（syntactic filters）对顶点集进行优化。语法过滤器的主要作用是将某一类或者某几类词性的字过滤出来作为顶点集。</p>\n<p>（2）关键短语抽取（keyphrase extration）<br>\n关键词抽取结束后，我们可以得到的 N 个关键词，在原始文本中相邻的关键词构成关键短语。因此，从 get_keyphrases 函数的源码中我们可以看到，它先调用 get_keywords 抽取关键词，然后分析关键词是否存在相邻的情况，最后确定哪些是关键短语。</p>\n<p>（3）关键句抽取（sentence extraction）<br>\n句子抽取任务主要针对的是自动摘要这个场景，将每一个 sentence 作为一个顶点，根据两个句子之间的内容重复程度来计算他们之间的 “相似度”，以这个相似度作为联系，由于不同句子之间相似度大小不一致，在这个场景下构建的是以相似度大小作为 edge 权重的有权图。</p>\n<h1 id=\"3-textrank算法实现\"><a class=\"markdownIt-Anchor\" href=\"#3-textrank算法实现\">#</a> 3 TextRank 算法实现</h1>\n<p>（1）基于 Textrank4zh 的 TextRank 算法实现</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding=utf-8</span><br><span class=\"line\">from textrank4zh import TextRank4Keyword, TextRank4Sentence</span><br><span class=\"line\">import jieba.analyse</span><br><span class=\"line\">from snownlp import SnowNLP</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"> </span><br><span class=\"line\">#关键词抽取</span><br><span class=\"line\">def keywords_extraction(text):</span><br><span class=\"line\">    tr4w = TextRank4Keyword(allow_speech_tags=[&#x27;n&#x27;, &#x27;nr&#x27;, &#x27;nrfg&#x27;, &#x27;ns&#x27;, &#x27;nt&#x27;, &#x27;nz&#x27;])</span><br><span class=\"line\">    # allow_speech_tags   --词性列表，用于过滤某些词性的词</span><br><span class=\"line\">    tr4w.analyze(text=text, window=2, lower=True, vertex_source=&#x27;all_filters&#x27;, edge_source=&#x27;no_stop_words&#x27;,</span><br><span class=\"line\">                 pagerank_config=&#123;&#x27;alpha&#x27;: 0.85, &#125;)</span><br><span class=\"line\">    # text    --  文本内容，字符串</span><br><span class=\"line\">    # window  --  窗口大小，int，用来构造单词之间的边。默认值为2</span><br><span class=\"line\">    # lower   --  是否将英文文本转换为小写，默认值为False</span><br><span class=\"line\">    # vertex_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点</span><br><span class=\"line\">    #                -- 默认值为`&#x27;all_filters&#x27;`，可选值为`&#x27;no_filter&#x27;, &#x27;no_stop_words&#x27;, &#x27;all_filters&#x27;</span><br><span class=\"line\">    # edge_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点之间的边</span><br><span class=\"line\">    #              -- 默认值为`&#x27;no_stop_words&#x27;`，可选值为`&#x27;no_filter&#x27;, &#x27;no_stop_words&#x27;, &#x27;all_filters&#x27;`。边的构造要结合`window`参数</span><br><span class=\"line\"> </span><br><span class=\"line\">    # pagerank_config  -- pagerank算法参数配置，阻尼系数为0.85</span><br><span class=\"line\">    keywords = tr4w.get_keywords(num=6, word_min_len=2)</span><br><span class=\"line\">    # num           --  返回关键词数量</span><br><span class=\"line\">    # word_min_len  --  词的最小长度，默认值为1</span><br><span class=\"line\">    return keywords</span><br></pre></td></tr></table></figure>\n<h1 id=\"关键短语抽取\"><a class=\"markdownIt-Anchor\" href=\"#关键短语抽取\">#</a> 关键短语抽取</h1>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">def keyphrases_extraction(text):</span><br><span class=\"line\">    tr4w = TextRank4Keyword()</span><br><span class=\"line\">    tr4w.analyze(text=text, window=2, lower=True, vertex_source=&#x27;all_filters&#x27;, edge_source=&#x27;no_stop_words&#x27;,</span><br><span class=\"line\">                 pagerank_config=&#123;&#x27;alpha&#x27;: 0.85, &#125;)</span><br><span class=\"line\">    keyphrases = tr4w.get_keyphrases(keywords_num=6, min_occur_num=1)</span><br><span class=\"line\">    # keywords_num    --  抽取的关键词数量</span><br><span class=\"line\">    # min_occur_num   --  关键短语在文中的最少出现次数</span><br><span class=\"line\">    return keyphrases</span><br></pre></td></tr></table></figure>\n<h1 id=\"关键句抽取\"><a class=\"markdownIt-Anchor\" href=\"#关键句抽取\">#</a> 关键句抽取</h1>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">def keysentences_extraction(text):</span><br><span class=\"line\">    tr4s = TextRank4Sentence()</span><br><span class=\"line\">    tr4s.analyze(text, lower=True, source=&#x27;all_filters&#x27;)</span><br><span class=\"line\">    # text    -- 文本内容，字符串</span><br><span class=\"line\">    # lower   -- 是否将英文文本转换为小写，默认值为False</span><br><span class=\"line\">    # source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来生成句子之间的相似度。</span><br><span class=\"line\">    # \t\t  -- 默认值为`&#x27;all_filters&#x27;`，可选值为`&#x27;no_filter&#x27;, &#x27;no_stop_words&#x27;, &#x27;all_filters&#x27;</span><br><span class=\"line\">    # sim_func -- 指定计算句子相似度的函数</span><br><span class=\"line\"> </span><br><span class=\"line\">    # 获取最重要的num个长度大于等于sentence_min_len的句子用来生成摘要</span><br><span class=\"line\">    keysentences = tr4s.get_key_sentences(num=3, sentence_min_len=6)</span><br><span class=\"line\">    return keysentences</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">def keywords_textrank(text):</span><br><span class=\"line\">    keywords = jieba.analyse.textrank(text, topK=6)</span><br><span class=\"line\">    return keywords</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">if __name__ == &quot;__main__&quot;:</span><br><span class=\"line\">    text = &quot;来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，&quot; \\</span><br><span class=\"line\">           &quot;我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、&quot; \\</span><br><span class=\"line\">           &quot;副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”&quot; \\</span><br><span class=\"line\">           &quot;据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，&quot; \\</span><br><span class=\"line\">           &quot;获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，&quot; \\</span><br><span class=\"line\">           &quot;国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，&quot; \\</span><br><span class=\"line\">           &quot;正式将该小行星命名为“周又元星”。&quot;</span><br><span class=\"line\">    #关键词抽取</span><br><span class=\"line\">    keywords=keywords_extraction(text)</span><br><span class=\"line\">    print(keywords)</span><br><span class=\"line\"> </span><br><span class=\"line\">    #关键短语抽取</span><br><span class=\"line\">    keyphrases=keyphrases_extraction(text)</span><br><span class=\"line\">    print(keyphrases)</span><br><span class=\"line\"> </span><br><span class=\"line\">    #关键句抽取</span><br><span class=\"line\">    keysentences=keysentences_extraction(text)</span><br><span class=\"line\">    print(keysentences)</span><br></pre></td></tr></table></figure>\n<p>运行：<br>\n<img src=\"../assets/Snipaste_2023-02-26_13-05-12.png\" alt=\"\"></p>\n<p>（2）基于 jieba 的 TextRank 算法实现</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if __name__ == &quot;__main__&quot;:</span><br><span class=\"line\">    text = &quot;来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，&quot; \\</span><br><span class=\"line\">           &quot;我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、&quot; \\</span><br><span class=\"line\">           &quot;副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”&quot; \\</span><br><span class=\"line\">           &quot;据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，&quot; \\</span><br><span class=\"line\">           &quot;获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，&quot; \\</span><br><span class=\"line\">           &quot;国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，&quot; \\</span><br><span class=\"line\">           &quot;正式将该小行星命名为“周又元星”。&quot;</span><br><span class=\"line\"> </span><br><span class=\"line\">    # 基于jieba的textrank算法实现</span><br><span class=\"line\">    keywords=keywords_textrank(text)</span><br><span class=\"line\">    print(keywords)</span><br></pre></td></tr></table></figure>\n<p>运行：小星星，中国，命名，天文家…</p>\n<h1 id=\"什么是-tf-idf-算法\"><a class=\"markdownIt-Anchor\" href=\"#什么是-tf-idf-算法\">#</a> 什么是 TF-IDF 算法？</h1>\n<p>TF（全称 TermFrequency），中文含义词频，简单理解就是关键词出现在网页当中的频次。</p>\n<p>IDF（全称 InverseDocumentFrequency），中文含义逆文档频率，简单来说就是该关键词出现在所有文档里面的一种数据集合。</p>\n<p>TF-IDF 用来评估字词对于文档集合中某一篇文档的重要程度。TF-IDF 的计算公式为：<br>\nTF-IDF = 某文档中某词或字出现的次数 / 该文档的总字数或总词数 * log（全部文档的个数 /（包含该词或字的文档的篇数）+1）</p>\n<h1 id=\"用户画像\"><a class=\"markdownIt-Anchor\" href=\"#用户画像\">#</a> 用户画像</h1>\n<p>用户标签化：这个过程就是需要研究用户对内容的喜好程度，用户喜欢的内容即当作用户喜好的标签。</p>\n<p>在用户行为记录表中，我们所记下用户的行为在此时就发挥出重要的作用了。用户的浏览（时长 / 频率）、点击、分享 / 收藏 / 关注、其他商业化或关键信息均不同程度的代表的用户对这个内容的喜好程度。</p>\n",
            "tags": [
                "推荐",
                "大数据",
                "hadoop"
            ]
        },
        {
            "id": "https://durtime.github.io/blog/posts/c4e7614/",
            "url": "https://durtime.github.io/blog/posts/c4e7614/",
            "title": "Tf-idf算法",
            "date_published": "2023-03-10T01:34:41.000Z",
            "content_html": "<h1 id=\"什么是-tf-idf-算法\"><a class=\"markdownIt-Anchor\" href=\"#什么是-tf-idf-算法\">#</a> 什么是 TF-IDF 算法？</h1>\n<p>TF（全称 TermFrequency），中文含义词频，简单理解就是关键词出现在网页当中的频次。</p>\n<p>IDF（全称 InverseDocumentFrequency），中文含义逆文档频率，简单来说就是该关键词出现在所有文档里面的一种数据集合。</p>\n<p>TF-IDF 用来评估字词对于文档集合中某一篇文档的重要程度。TF-IDF 的计算公式为：<br>\nTF-IDF = 某文档中某词或字出现的次数 / 该文档的总字数或总词数 * log（全部文档的个数 /（包含该词或字的文档的篇数）+1）<br>\nTF-IDF 的思想比较简单，但是却非常实用。然而这种方法还是存在着数据稀疏的问题，也没有考虑字的前后信息。</p>\n<p>在信息检索中，tf-idf 或 TFIDF（术语频率 – 逆文档频率的缩写）是一种数字统计，旨在反映单词对集合或语料库中的文档的重要程度。</p>\n<p>它经常被用作搜索信息检索，文本挖掘和用户建模的加权因子。tf-idf 值按比例增加一个单词出现在文档中的次数，并被包含该单词的语料库中的文档数量所抵消，这有助于调整某些单词在一般情况下更频繁出现的事实。Tf-idf 是当今最受欢迎的术语加权方案之一；数字图书馆中 83％的基于文本的推荐系统使用 tf-idf。比如关键词 “中国” 在 A 网页里面出现了 100 次，那么它的 TF 值则是 100 次（词频），假设搜索引擎所收录的所有网页里面有 1 亿网页包含 “中国” 该关键词，那么 IDF 将由 IDF 公式计算出它对应的数据值。统一来理解则是 TF 是计算自己网页内的关键词频次，而 TDF 是计算所有文档里面包含该关键词的一种概率数值。</p>\n<p>搜索引擎经常使用 tf-idf 加权方案的变体作为在给定用户查询的情况下对文档的相关性进行评分和排序的中心工具。tf-idf 可以成功地用于各种主题领域的停用词过滤，包括文本摘要和分类。</p>\n<h1 id=\"tf-term-frequency-单词频率\"><a class=\"markdownIt-Anchor\" href=\"#tf-term-frequency-单词频率\">#</a> TF （Term Frequency）—— “单词频率”</h1>\n<p>意思就是说，我们计算一个查询关键字中某一个单词在目标文档中出现的次数。举例说来，如果我们要查询 “Car Insurance”，那么对于每一个文档，我们都计算 “Car” 这个单词在其中出现了多少次，“Insurance” 这个单词在其中出现了多少次。这个就是 TF 的计算方法。</p>\n<p>TF 背后的隐含的假设是，查询关键字中的单词应该相对于其他单词更加重要，而查询关键字中的单词相对文档的重要程度，即查询关键字中的单词与文档的的相关度，与单词在文档中出现的次数成正比。比如，“Car” 这个单词在文档 A 里出现了 5 次，而在文档 B 里出现了 20 次，那么 TF 计算就认为文档 B 可能更相关。</p>\n<p>然而，信息检索工作者很快就发现，仅有 TF 不能比较完整地描述文档的相关度。因为语言的因素，有一些单词可能会比较自然地在很多文档中反复出现，比如英语中的 “The”、“An”、“But” 等等。这些词大多起到了链接语句的作用，是保持语言连贯不可或缺的部分。然而，如果我们要搜索 “How to Build A Car” 这个关键词，其中的 “How”、“To” 以及 “A” 都极可能在绝大多数的文档中出现，这个时候 TF 就无法帮助我们区分文档的相关度了。</p>\n<h1 id=\"idfinverse-document-frequency-逆文档频率\"><a class=\"markdownIt-Anchor\" href=\"#idfinverse-document-frequency-逆文档频率\">#</a> IDF（Inverse Document Frequency）—— “逆文档频率”</h1>\n<p>就在这样的情况下应运而生。这里面的思路其实很简单，那就是我们需要去 “惩罚”（Penalize）那些出现在太多文档中的单词。</p>\n<p>也就是说，真正携带 “相关” 信息的单词仅仅出现在相对比较少，有时候可能是极少数的文档里。这个信息，很容易用 “文档频率” 来计算，也就是，有多少文档涵盖了这个单词。很明显，如果有太多文档都涵盖了某个单词，这个单词也就越不重要，或者说是这个单词就越没有信息量。因此，我们需要对 TF 的值进行修正，而 IDF 的想法是用 DF（文档频率） 的倒数来进行修正。倒数的应用正好表达了这样的思想，DF 值越大越不重要。</p>\n<p>让我们从一个实例开始讲起。假定现在有一篇长文《中国的蜜蜂养殖》，我们准备用计算机提取它的关键词。</p>\n<p>一个容易想到的思路，就是找到出现次数最多的词。如果某个词很重要，它应该在这篇文章中多次出现。于是，我们进行 &quot;词频&quot;（Term Frequency，缩写为 TF）统计。</p>\n<p>结果你肯定猜到了，出现次数最多的词是 ----“的”、“是”、“在”---- 这一类最常用的词。它们叫做 &quot;停用词&quot;（stop words），表示对找到结果毫无帮助、必须过滤掉的词。</p>\n<p>假设我们把它们都过滤掉了，只考虑剩下的有实际意义的词。这样又会遇到了另一个问题，我们可能发现 &quot;中国&quot;、“蜜蜂”、&quot;养殖&quot; 这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？</p>\n<p>显然不是这样。因为 &quot;中国&quot; 是很常见的词，相对而言，&quot;蜜蜂&quot; 和 &quot;养殖&quot; 不那么常见。如果这三个词在一篇文章的出现次数一样多，有理由认为，“蜜蜂 &quot;和&quot; 养殖 &quot;的重要程度要大于&quot; 中国”，也就是说，在关键词排序上面，&quot;蜜蜂&quot; 和 &quot;养殖&quot; 应该排在 &quot;中国&quot; 的前面。</p>\n<p>所以，我们需要一个重要性调整系数，衡量一个词是不是常见词。如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。</p>\n<p>用统计学语言表达，就是在词频的基础上，要对每个词分配一个 &quot;重要性&quot; 权重。最常见的词（“的”、“是”、“在”）给予最小的权重，较常见的词（“中国”）给予较小的权重，较少见的词（“蜜蜂”、“养殖”）给予较大的权重。这个权重叫做 &quot;逆文档频率&quot;（Inverse Document Frequency，缩写为 IDF），它的大小与一个词的常见程度成反比。</p>\n<p>知道了 &quot;词频&quot;（TF）和 &quot;逆文档频率&quot;（IDF）以后，将这两个值相乘，就得到了一个词的 TF-IDF 值。某个词对文章的重要性越高，它的 TF-IDF 值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。</p>\n<h1 id=\"算法的细节\"><a class=\"markdownIt-Anchor\" href=\"#算法的细节\">#</a> 算法的细节</h1>\n<p><img src=\"../assets/Snipaste_2023-02-26_13-21-36.png\" alt=\"算法的细节\"></p>\n<p><img src=\"../assets/Snipaste_2023-02-26_13-24-00.png\" alt=\"算法的细节\"><br>\n可以看到，TF-IDF 与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的 TF-IDF 值，然后按降序排列，取排在最前面的几个词。</p>\n<p>还是以《中国的蜜蜂养殖》为例，假定该文长度为 1000 个词，“中国”、“蜜蜂”、“养殖 &quot;各出现 20 次，则这三个词的&quot; 词频”（TF）都为 0.02。然后，搜索 Google 发现，包含 &quot;的&quot; 字的网页共有 250 亿张，假定这就是中文网页总数。包含 &quot;中国&quot; 的网页共有 62.3 亿张，包含 &quot;蜜蜂&quot; 的网页为 0.484 亿张，包含 &quot;养殖&quot; 的网页为 0.973 亿张。则它们的逆文档频率（IDF）和 TF-IDF 如下：</p>\n<p><img src=\"../assets/Snipaste_2023-02-26_13-24-41.png\" alt=\"算法的细节\"></p>\n<p>log (250 亿 /（62.3 亿 + 1）) 约等于 0.603<br>\n “中国” 这个词的词频怎么算的就不知道了</p>\n<p>从上表可见，&quot;蜜蜂&quot; 的 TF-IDF 值最高，&quot;养殖&quot; 其次，&quot;中国&quot; 最低。（如果还计算 &quot;的&quot; 字的 TF-IDF，那将是一个极其接近 0 的值。）所以，如果只选择一个词，&quot;蜜蜂&quot; 就是这篇文章的关键词。</p>\n<p>除了自动提取关键词，TF-IDF 算法还可以用于许多别的地方。比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（“中国”、“蜜蜂”、“养殖”）的 TF-IDF，将它们相加，就可以得到整个文档的 TF-IDF。这个值最高的文档就是与搜索词最相关的文档。</p>\n<p>TF-IDF 算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以 &quot;词频&quot; 衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。这一点对于 SEO 特别重要）</p>\n<p><strong>TF-IDF 算法思想：</strong></p>\n<p>TF-IDF 的核心思想是通过该算法进行有效的计算网页的核心关键词。虽然语义分析以及中文分词能够简单的计算出页面的关键词主题，但是由于互联网内容信息重复度较大，同一个内容单纯从分词角度来讲是不足以满足搜索引擎针对网页的内容是否更加符合用户的需求。而 TF-IDF 则可以用过算法公式来计算用户搜索词与网页之间的相似度。</p>\n<p>比如网页标题 “小黑的同桌叫马天”，这里面 “的”、“叫” 在搜索引擎里面一般都称为停用词 (stop words)，也就是无意义词。而去掉这些词剩下的词则是小黑、同桌、马天。根据分词原理，这三个词都是名词，那么作为用户而言去看这个标题明显知道是阐述马天是网页的核心关键词，但是对于搜索引擎来说并不能深刻的理解该网页的核心关键词。对于这三个词，一般我们都有一个词的重要程度系数。从常见度来说，越常见的东西则不重要，反之越不常见越重要。那么搜索引擎是如何知道该词的常见程度呢？可以通过相关搜索结果数来计算关键词的重要度。</p>\n<p><strong>TF-IDF 算法应用：</strong><br>\n关于 TF-IDF 的算法实战应用，最常见的方式则是利用 TF-IDF 算法的计算方式来进行定位网页的核心词，从而网站大量提升关键词排名。首先我们要理解真正原创文章的含义，真正的内容原创有两种，一种是网页内容与总语料库文档不重合，另一种则是关键词与该关键词的相关结果文档不重合。而 TF-IDF 最佳的运用方式则是可以采用换汤不换药的操作方式来进行关键词排名。比如优化一个关键词 “山药的功效与作用”，那么我们可以去抄袭一篇 “人参的功效与作用” 的文章，并且替换网页里面的所有人参关键词，尽管这篇文章在人参里面是重复性很高的文章。但是在山药的功效与作用里面它就是独一无二的。并且刻意增加山药文章里面的 TF 值，让搜索引擎认定该网页的核心关键词。</p>\n<p>镜像站专门干这事！</p>\n<p>网站镜像是通过 TF-IDF 算法应用的经典案例，内容全部抄袭，网页标题（title）与文章标题不同，目的就是用来提升网页的点击率。并且文章标题我们可以发现互博国际该关键词是一个由多个词组成的词组，通过分词符号可以让互博国际变成一个关键词（词组形成关键词）。并且在网页内容里面，自然的分布关键词的频次（TF），从而达到关键词的强调性，即使内容在其他文章里面出现，但是在互博国际里面，该内容则是独一无二的原创（不重合）。为了提升网页的点击率，我们可以将网页的标题（title）写的更加规范，这样排名一旦上来，还有利于用户的点击，从而提升网页关键词的排名更佳状态。</p>\n<p>以上参考：SEO 算法篇，TFIDF 算法讲解及在 SEO 中的利用</p>\n<p><strong>代码</strong> 看看就行（没必要会写，会复制粘贴即可）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class=\"line\">tfidf_vec = TfidfVectorizer()</span><br><span class=\"line\"># stop words自定义停用词表，为列表List类型</span><br><span class=\"line\"># token_pattern过滤规则，正则表达式，如r&quot;(?u)bw+b</span><br><span class=\"line\"># max_df=0.5，代表一个单词在 50% 的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计</span><br><span class=\"line\">documents = [</span><br><span class=\"line\">    &#x27;this is the bayes document&#x27;,</span><br><span class=\"line\">    &#x27;this is the second second document&#x27;,</span><br><span class=\"line\">    &#x27;and the third one&#x27;,</span><br><span class=\"line\">    &#x27;is this the document&#x27;</span><br><span class=\"line\">]</span><br><span class=\"line\">tfidf_matrix = tfidf_vec.fit_transform(documents)</span><br><span class=\"line\"># 拟合模型，并返回文本矩阵  表示了每个单词在每个文档中的 TF-IDF 值</span><br><span class=\"line\">print(&#x27;输出每个单词在每个文档中的 TF-IDF 值，向量里的顺序是按照词语的 id 顺序来的:&#x27;, &#x27;\\n&#x27;, tfidf_matrix.toarray())</span><br><span class=\"line\">print(&#x27;不重复的词:&#x27;, tfidf_vec.get_feature_names())</span><br><span class=\"line\">print(&#x27;输出每个单词对应的 id 值:&#x27;, tfidf_vec.vocabulary_)</span><br><span class=\"line\">print(&#x27;返回idf值:&#x27;, tfidf_vec.idf_)</span><br><span class=\"line\">print(&#x27;返回停用词表:&#x27;, tfidf_vec.stop_words_)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>",
            "tags": [
                "推荐",
                "算法",
                "大数据"
            ]
        },
        {
            "id": "https://durtime.github.io/blog/posts/65ad05d8/",
            "url": "https://durtime.github.io/blog/posts/65ad05d8/",
            "title": "一梦文章推荐业务(一)",
            "date_published": "2022-05-28T14:36:02.000Z",
            "content_html": "<h1 id=\"1-数据库迁移需求\"><a class=\"markdownIt-Anchor\" href=\"#1-数据库迁移需求\">#</a> 1 数据库迁移需求</h1>\n<p>业务 mysql 数据库中的数据，会同步到我们的 hadoop 的 hive 数据仓库中。</p>\n<ul>\n<li>为了避免直接连接、操作业务数据</li>\n<li>同步一份数据在集群中方便进行数据分析操作</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; show databases;</span><br><span class=\"line\">OK</span><br><span class=\"line\">default</span><br><span class=\"line\">profile</span><br><span class=\"line\">toutiao</span><br><span class=\"line\">Time taken: 0.017 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure>\n<p>创建 hive 业务数据库 onedream</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create database if not exists onedream comment &quot;user,news information of onedream mysql&quot; location &#x27;/user/hive/warehouse/onedream.db/&#x27;;</span><br></pre></td></tr></table></figure>\n<h1 id=\"2-sqoop导入\"><a class=\"markdownIt-Anchor\" href=\"#2-sqoop导入\">#</a> 2 sqoop 导入</h1>\n<p>用户：基本信息，关注，收藏，搜索，订阅（设置选择喜好频道）<br>\n文章：分类，文章</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">array=(user_profile user_basic news_user_channel news_channel user_follow user_blacklist user_search news_collection news_article_basic news_article_content news_read news_article_statistic user_material)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">for table_name in $&#123;array[@]&#125;;</span><br><span class=\"line\">do</span><br><span class=\"line\">    sqoop import \\</span><br><span class=\"line\">        --connect jdbc:mysql://192.168.19.137/onedream \\</span><br><span class=\"line\">        --username root \\</span><br><span class=\"line\">        --password password \\</span><br><span class=\"line\">        --table $table_name \\</span><br><span class=\"line\">        --m 5 \\</span><br><span class=\"line\">        --hive-home /root/bigdata/hive \\</span><br><span class=\"line\">        --hive-import \\</span><br><span class=\"line\">        --create-hive-table  \\</span><br><span class=\"line\">        --hive-drop-import-delims \\</span><br><span class=\"line\">        --warehouse-dir /user/hive/warehouse/onedream.db \\</span><br><span class=\"line\">        --hive-table onedream.$table_name</span><br><span class=\"line\">done</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Mysql 导入对应 hive 类型:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL(bigint) --&gt; Hive(bigint) </span><br><span class=\"line\">MySQL(tinyint) --&gt; Hive(tinyint) </span><br><span class=\"line\">MySQL(int) --&gt; Hive(int) </span><br><span class=\"line\">MySQL(double) --&gt; Hive(double) </span><br><span class=\"line\">MySQL(bit) --&gt; Hive(boolean) </span><br><span class=\"line\">MySQL(varchar) --&gt; Hive(string) </span><br><span class=\"line\">MySQL(decimal) --&gt; Hive(double) </span><br><span class=\"line\">MySQL(date/timestamp) --&gt; Hive(string)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"3-tfidf模型的训练\"><a class=\"markdownIt-Anchor\" href=\"#3-tfidf模型的训练\">#</a> 3 TFIDF 模型的训练</h1>\n<p>步骤：<br>\n1、读取 N 篇文章数据<br>\n 2、文章数据进行分词处理<br>\n 3、TFIDF 模型训练保存，spark 使用 count 与 idf 进行计算<br>\n 4、利用模型计算 N 篇文章数据的 TFIDF 值</p>\n<h2 id=\"分词\"><a class=\"markdownIt-Anchor\" href=\"#分词\">#</a> 分词</h2>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 分词</span><br><span class=\"line\">def segmentation(partition):</span><br><span class=\"line\">    import os</span><br><span class=\"line\">    import re</span><br><span class=\"line\"></span><br><span class=\"line\">    import jieba</span><br><span class=\"line\">    import jieba.analyse</span><br><span class=\"line\">    import jieba.posseg as pseg</span><br><span class=\"line\">    import codecs</span><br><span class=\"line\"></span><br><span class=\"line\">    abspath = &quot;/root/words&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    # 结巴加载用户词典</span><br><span class=\"line\">    userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)</span><br><span class=\"line\">    jieba.load_userdict(userDict_path)</span><br><span class=\"line\"></span><br><span class=\"line\">    # 停用词文本</span><br><span class=\"line\">    stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    def get_stopwords_list():</span><br><span class=\"line\">        &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot;</span><br><span class=\"line\">        stopwords_list = [i.strip()</span><br><span class=\"line\">                          for i in codecs.open(stopwords_path).readlines()]</span><br><span class=\"line\">        return stopwords_list</span><br><span class=\"line\"></span><br><span class=\"line\">    # 所有的停用词列表</span><br><span class=\"line\">    stopwords_list = get_stopwords_list()</span><br><span class=\"line\"></span><br><span class=\"line\">    # 分词</span><br><span class=\"line\">    def cut_sentence(sentence):</span><br><span class=\"line\">        &quot;&quot;&quot;对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词&quot;&quot;&quot;</span><br><span class=\"line\">        # print(sentence,&quot;*&quot;*100)</span><br><span class=\"line\">        # eg:[pair(&#x27;今天&#x27;, &#x27;t&#x27;), pair(&#x27;有&#x27;, &#x27;d&#x27;), pair(&#x27;雾&#x27;, &#x27;n&#x27;), pair(&#x27;霾&#x27;, &#x27;g&#x27;)]</span><br><span class=\"line\">        seg_list = pseg.lcut(sentence)</span><br><span class=\"line\">        seg_list = [i for i in seg_list if i.flag not in stopwords_list]</span><br><span class=\"line\">        filtered_words_list = []</span><br><span class=\"line\">        for seg in seg_list:</span><br><span class=\"line\">            # print(seg)</span><br><span class=\"line\">            if len(seg.word) &lt;= 1:</span><br><span class=\"line\">                continue</span><br><span class=\"line\">            elif seg.flag == &quot;eng&quot;:</span><br><span class=\"line\">                if len(seg.word) &lt;= 2:</span><br><span class=\"line\">                    continue</span><br><span class=\"line\">                else:</span><br><span class=\"line\">                    filtered_words_list.append(seg.word)</span><br><span class=\"line\">            elif seg.flag.startswith(&quot;n&quot;):</span><br><span class=\"line\">                filtered_words_list.append(seg.word)</span><br><span class=\"line\">            elif seg.flag in [&quot;x&quot;, &quot;eng&quot;]:  # 是自定一个词语或者是英文单词</span><br><span class=\"line\">                filtered_words_list.append(seg.word)</span><br><span class=\"line\">        return filtered_words_list</span><br><span class=\"line\"></span><br><span class=\"line\">    for row in partition:</span><br><span class=\"line\">        sentence = re.sub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, row.sentence)    # 替换掉标签数据</span><br><span class=\"line\">        words = cut_sentence(sentence)</span><br><span class=\"line\">        yield row.article_id, row.channel_id, words</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>训练模型，得到每个文章词的频率 Counts 结果</p>\n<h1 id=\"词语与词频统计\"><a class=\"markdownIt-Anchor\" href=\"#词语与词频统计\">#</a> 词语与词频统计</h1>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 词语与词频统计</span><br><span class=\"line\">from pyspark.ml.feature import CountVectorizer</span><br><span class=\"line\"># 总词汇的大小，文本中必须出现的次数</span><br><span class=\"line\">cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;countFeatures&quot;, vocabSize=200*10000, minDF=1.0)</span><br><span class=\"line\"># 训练词频统计模型</span><br><span class=\"line\">cv_model = cv.fit(words_df)</span><br><span class=\"line\">cv_model.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)</span><br></pre></td></tr></table></figure>",
            "tags": [
                "推荐",
                "大数据",
                "hadoop"
            ]
        }
    ]
}