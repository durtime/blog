{
    "version": "https://jsonfeed.org/version/1",
    "title": "Durtime • All posts by \"hadoop\" tag",
    "description": "The aroma of black tea no longer exists",
    "home_page_url": "https://durtime.github.io/blog",
    "items": [
        {
            "id": "https://durtime.github.io/blog/posts/1fd8e8b9/",
            "url": "https://durtime.github.io/blog/posts/1fd8e8b9/",
            "title": "推荐业务(一)",
            "date_published": "2023-03-12T01:34:41.000Z",
            "content_html": "<h1 id=\"1-离线画像业务\"><a class=\"markdownIt-Anchor\" href=\"#1-离线画像业务\">#</a> 1 离线画像业务</h1>\n<p>画像的构建作为推荐系统非常重要的环节，画像可以作为整个产品的推荐或者营销重要依据。需要通过各种方法来构建。</p>\n<p>文章内容标签化：内容标签化，根据内容定性的制定一系列标签，这些标签可以是描述性标签。针对于文章就是文章相关的内容词语。</p>\n<p>文章的关键词、主题词</p>\n<h1 id=\"2-textrank算法\"><a class=\"markdownIt-Anchor\" href=\"#2-textrank算法\">#</a> 2 TextRank 算法</h1>\n<p>TextRank 算法是一种基于图的用于关键词抽取和文档摘要的排序算法，由谷歌的网页重要性排序算法 PageRank 算法改进而来，它利用一篇文档内部的词语间的共现信息 (语义) 便可以抽取关键词，它能够从一个给定的文本中抽取出该文本的关键词、关键词组，并使用抽取式的自动文摘方法抽取出该文本的关键句。</p>\n<p>TextRank 算法的基本思想是将文档看作一个词的网络，该网络中的链接表示词与词之间的语义关系。</p>\n<p>TextRank 算法主要包括：关键词抽取、关键短语抽取、关键句抽取。</p>\n<p>（1）关键词抽取（keyword extraction）<br>\n关键词抽取是指从文本中确定一些能够描述文档含义的术语的过程。对关键词抽取而言，用于构建顶点集的文本单元可以是句子中的一个或多个字；根据这些字之间的关系（比如：在一个框中同时出现）构建边。根据任务的需要，可以使用语法过滤器（syntactic filters）对顶点集进行优化。语法过滤器的主要作用是将某一类或者某几类词性的字过滤出来作为顶点集。</p>\n<p>（2）关键短语抽取（keyphrase extration）<br>\n关键词抽取结束后，我们可以得到的 N 个关键词，在原始文本中相邻的关键词构成关键短语。因此，从 get_keyphrases 函数的源码中我们可以看到，它先调用 get_keywords 抽取关键词，然后分析关键词是否存在相邻的情况，最后确定哪些是关键短语。</p>\n<p>（3）关键句抽取（sentence extraction）<br>\n句子抽取任务主要针对的是自动摘要这个场景，将每一个 sentence 作为一个顶点，根据两个句子之间的内容重复程度来计算他们之间的 “相似度”，以这个相似度作为联系，由于不同句子之间相似度大小不一致，在这个场景下构建的是以相似度大小作为 edge 权重的有权图。</p>\n<h1 id=\"3-textrank算法实现\"><a class=\"markdownIt-Anchor\" href=\"#3-textrank算法实现\">#</a> 3 TextRank 算法实现</h1>\n<p>（1）基于 Textrank4zh 的 TextRank 算法实现</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding=utf-8</span><br><span class=\"line\">from textrank4zh import TextRank4Keyword, TextRank4Sentence</span><br><span class=\"line\">import jieba.analyse</span><br><span class=\"line\">from snownlp import SnowNLP</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"> </span><br><span class=\"line\">#关键词抽取</span><br><span class=\"line\">def keywords_extraction(text):</span><br><span class=\"line\">    tr4w = TextRank4Keyword(allow_speech_tags=[&#x27;n&#x27;, &#x27;nr&#x27;, &#x27;nrfg&#x27;, &#x27;ns&#x27;, &#x27;nt&#x27;, &#x27;nz&#x27;])</span><br><span class=\"line\">    # allow_speech_tags   --词性列表，用于过滤某些词性的词</span><br><span class=\"line\">    tr4w.analyze(text=text, window=2, lower=True, vertex_source=&#x27;all_filters&#x27;, edge_source=&#x27;no_stop_words&#x27;,</span><br><span class=\"line\">                 pagerank_config=&#123;&#x27;alpha&#x27;: 0.85, &#125;)</span><br><span class=\"line\">    # text    --  文本内容，字符串</span><br><span class=\"line\">    # window  --  窗口大小，int，用来构造单词之间的边。默认值为2</span><br><span class=\"line\">    # lower   --  是否将英文文本转换为小写，默认值为False</span><br><span class=\"line\">    # vertex_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点</span><br><span class=\"line\">    #                -- 默认值为`&#x27;all_filters&#x27;`，可选值为`&#x27;no_filter&#x27;, &#x27;no_stop_words&#x27;, &#x27;all_filters&#x27;</span><br><span class=\"line\">    # edge_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点之间的边</span><br><span class=\"line\">    #              -- 默认值为`&#x27;no_stop_words&#x27;`，可选值为`&#x27;no_filter&#x27;, &#x27;no_stop_words&#x27;, &#x27;all_filters&#x27;`。边的构造要结合`window`参数</span><br><span class=\"line\"> </span><br><span class=\"line\">    # pagerank_config  -- pagerank算法参数配置，阻尼系数为0.85</span><br><span class=\"line\">    keywords = tr4w.get_keywords(num=6, word_min_len=2)</span><br><span class=\"line\">    # num           --  返回关键词数量</span><br><span class=\"line\">    # word_min_len  --  词的最小长度，默认值为1</span><br><span class=\"line\">    return keywords</span><br></pre></td></tr></table></figure>\n<h1 id=\"关键短语抽取\"><a class=\"markdownIt-Anchor\" href=\"#关键短语抽取\">#</a> 关键短语抽取</h1>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">def keyphrases_extraction(text):</span><br><span class=\"line\">    tr4w = TextRank4Keyword()</span><br><span class=\"line\">    tr4w.analyze(text=text, window=2, lower=True, vertex_source=&#x27;all_filters&#x27;, edge_source=&#x27;no_stop_words&#x27;,</span><br><span class=\"line\">                 pagerank_config=&#123;&#x27;alpha&#x27;: 0.85, &#125;)</span><br><span class=\"line\">    keyphrases = tr4w.get_keyphrases(keywords_num=6, min_occur_num=1)</span><br><span class=\"line\">    # keywords_num    --  抽取的关键词数量</span><br><span class=\"line\">    # min_occur_num   --  关键短语在文中的最少出现次数</span><br><span class=\"line\">    return keyphrases</span><br></pre></td></tr></table></figure>\n<h1 id=\"关键句抽取\"><a class=\"markdownIt-Anchor\" href=\"#关键句抽取\">#</a> 关键句抽取</h1>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">def keysentences_extraction(text):</span><br><span class=\"line\">    tr4s = TextRank4Sentence()</span><br><span class=\"line\">    tr4s.analyze(text, lower=True, source=&#x27;all_filters&#x27;)</span><br><span class=\"line\">    # text    -- 文本内容，字符串</span><br><span class=\"line\">    # lower   -- 是否将英文文本转换为小写，默认值为False</span><br><span class=\"line\">    # source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来生成句子之间的相似度。</span><br><span class=\"line\">    # \t\t  -- 默认值为`&#x27;all_filters&#x27;`，可选值为`&#x27;no_filter&#x27;, &#x27;no_stop_words&#x27;, &#x27;all_filters&#x27;</span><br><span class=\"line\">    # sim_func -- 指定计算句子相似度的函数</span><br><span class=\"line\"> </span><br><span class=\"line\">    # 获取最重要的num个长度大于等于sentence_min_len的句子用来生成摘要</span><br><span class=\"line\">    keysentences = tr4s.get_key_sentences(num=3, sentence_min_len=6)</span><br><span class=\"line\">    return keysentences</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">def keywords_textrank(text):</span><br><span class=\"line\">    keywords = jieba.analyse.textrank(text, topK=6)</span><br><span class=\"line\">    return keywords</span><br><span class=\"line\"> </span><br><span class=\"line\"> </span><br><span class=\"line\">if __name__ == &quot;__main__&quot;:</span><br><span class=\"line\">    text = &quot;来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，&quot; \\</span><br><span class=\"line\">           &quot;我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、&quot; \\</span><br><span class=\"line\">           &quot;副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”&quot; \\</span><br><span class=\"line\">           &quot;据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，&quot; \\</span><br><span class=\"line\">           &quot;获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，&quot; \\</span><br><span class=\"line\">           &quot;国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，&quot; \\</span><br><span class=\"line\">           &quot;正式将该小行星命名为“周又元星”。&quot;</span><br><span class=\"line\">    #关键词抽取</span><br><span class=\"line\">    keywords=keywords_extraction(text)</span><br><span class=\"line\">    print(keywords)</span><br><span class=\"line\"> </span><br><span class=\"line\">    #关键短语抽取</span><br><span class=\"line\">    keyphrases=keyphrases_extraction(text)</span><br><span class=\"line\">    print(keyphrases)</span><br><span class=\"line\"> </span><br><span class=\"line\">    #关键句抽取</span><br><span class=\"line\">    keysentences=keysentences_extraction(text)</span><br><span class=\"line\">    print(keysentences)</span><br></pre></td></tr></table></figure>\n<p>运行：<br>\n<img src=\"../assets/Snipaste_2023-02-26_13-05-12.png\" alt=\"\"></p>\n<p>（2）基于 jieba 的 TextRank 算法实现</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if __name__ == &quot;__main__&quot;:</span><br><span class=\"line\">    text = &quot;来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，&quot; \\</span><br><span class=\"line\">           &quot;我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、&quot; \\</span><br><span class=\"line\">           &quot;副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”&quot; \\</span><br><span class=\"line\">           &quot;据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，&quot; \\</span><br><span class=\"line\">           &quot;获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，&quot; \\</span><br><span class=\"line\">           &quot;国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，&quot; \\</span><br><span class=\"line\">           &quot;正式将该小行星命名为“周又元星”。&quot;</span><br><span class=\"line\"> </span><br><span class=\"line\">    # 基于jieba的textrank算法实现</span><br><span class=\"line\">    keywords=keywords_textrank(text)</span><br><span class=\"line\">    print(keywords)</span><br></pre></td></tr></table></figure>\n<p>运行：小星星，中国，命名，天文家…</p>\n<h1 id=\"什么是-tf-idf-算法\"><a class=\"markdownIt-Anchor\" href=\"#什么是-tf-idf-算法\">#</a> 什么是 TF-IDF 算法？</h1>\n<p>TF（全称 TermFrequency），中文含义词频，简单理解就是关键词出现在网页当中的频次。</p>\n<p>IDF（全称 InverseDocumentFrequency），中文含义逆文档频率，简单来说就是该关键词出现在所有文档里面的一种数据集合。</p>\n<p>TF-IDF 用来评估字词对于文档集合中某一篇文档的重要程度。TF-IDF 的计算公式为：<br>\nTF-IDF = 某文档中某词或字出现的次数 / 该文档的总字数或总词数 * log（全部文档的个数 /（包含该词或字的文档的篇数）+1）</p>\n<h1 id=\"用户画像\"><a class=\"markdownIt-Anchor\" href=\"#用户画像\">#</a> 用户画像</h1>\n<p>用户标签化：这个过程就是需要研究用户对内容的喜好程度，用户喜欢的内容即当作用户喜好的标签。</p>\n<p>在用户行为记录表中，我们所记下用户的行为在此时就发挥出重要的作用了。用户的浏览（时长 / 频率）、点击、分享 / 收藏 / 关注、其他商业化或关键信息均不同程度的代表的用户对这个内容的喜好程度。</p>\n",
            "tags": [
                "推荐",
                "大数据",
                "hadoop"
            ]
        },
        {
            "id": "https://durtime.github.io/blog/posts/65ad05d8/",
            "url": "https://durtime.github.io/blog/posts/65ad05d8/",
            "title": "一梦文章推荐业务(一)",
            "date_published": "2022-05-28T14:36:02.000Z",
            "content_html": "<h1 id=\"1-数据库迁移需求\"><a class=\"markdownIt-Anchor\" href=\"#1-数据库迁移需求\">#</a> 1 数据库迁移需求</h1>\n<p>业务 mysql 数据库中的数据，会同步到我们的 hadoop 的 hive 数据仓库中。</p>\n<ul>\n<li>为了避免直接连接、操作业务数据</li>\n<li>同步一份数据在集群中方便进行数据分析操作</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; show databases;</span><br><span class=\"line\">OK</span><br><span class=\"line\">default</span><br><span class=\"line\">profile</span><br><span class=\"line\">toutiao</span><br><span class=\"line\">Time taken: 0.017 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure>\n<p>创建 hive 业务数据库 onedream</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create database if not exists onedream comment &quot;user,news information of onedream mysql&quot; location &#x27;/user/hive/warehouse/onedream.db/&#x27;;</span><br></pre></td></tr></table></figure>\n<h1 id=\"2-sqoop导入\"><a class=\"markdownIt-Anchor\" href=\"#2-sqoop导入\">#</a> 2 sqoop 导入</h1>\n<p>用户：基本信息，关注，收藏，搜索，订阅（设置选择喜好频道）<br>\n文章：分类，文章</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">array=(user_profile user_basic news_user_channel news_channel user_follow user_blacklist user_search news_collection news_article_basic news_article_content news_read news_article_statistic user_material)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">for table_name in $&#123;array[@]&#125;;</span><br><span class=\"line\">do</span><br><span class=\"line\">    sqoop import \\</span><br><span class=\"line\">        --connect jdbc:mysql://192.168.19.137/onedream \\</span><br><span class=\"line\">        --username root \\</span><br><span class=\"line\">        --password password \\</span><br><span class=\"line\">        --table $table_name \\</span><br><span class=\"line\">        --m 5 \\</span><br><span class=\"line\">        --hive-home /root/bigdata/hive \\</span><br><span class=\"line\">        --hive-import \\</span><br><span class=\"line\">        --create-hive-table  \\</span><br><span class=\"line\">        --hive-drop-import-delims \\</span><br><span class=\"line\">        --warehouse-dir /user/hive/warehouse/onedream.db \\</span><br><span class=\"line\">        --hive-table onedream.$table_name</span><br><span class=\"line\">done</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Mysql 导入对应 hive 类型:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL(bigint) --&gt; Hive(bigint) </span><br><span class=\"line\">MySQL(tinyint) --&gt; Hive(tinyint) </span><br><span class=\"line\">MySQL(int) --&gt; Hive(int) </span><br><span class=\"line\">MySQL(double) --&gt; Hive(double) </span><br><span class=\"line\">MySQL(bit) --&gt; Hive(boolean) </span><br><span class=\"line\">MySQL(varchar) --&gt; Hive(string) </span><br><span class=\"line\">MySQL(decimal) --&gt; Hive(double) </span><br><span class=\"line\">MySQL(date/timestamp) --&gt; Hive(string)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"3-tfidf模型的训练\"><a class=\"markdownIt-Anchor\" href=\"#3-tfidf模型的训练\">#</a> 3 TFIDF 模型的训练</h1>\n<p>步骤：<br>\n1、读取 N 篇文章数据<br>\n 2、文章数据进行分词处理<br>\n 3、TFIDF 模型训练保存，spark 使用 count 与 idf 进行计算<br>\n 4、利用模型计算 N 篇文章数据的 TFIDF 值</p>\n<h2 id=\"分词\"><a class=\"markdownIt-Anchor\" href=\"#分词\">#</a> 分词</h2>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 分词</span><br><span class=\"line\">def segmentation(partition):</span><br><span class=\"line\">    import os</span><br><span class=\"line\">    import re</span><br><span class=\"line\"></span><br><span class=\"line\">    import jieba</span><br><span class=\"line\">    import jieba.analyse</span><br><span class=\"line\">    import jieba.posseg as pseg</span><br><span class=\"line\">    import codecs</span><br><span class=\"line\"></span><br><span class=\"line\">    abspath = &quot;/root/words&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    # 结巴加载用户词典</span><br><span class=\"line\">    userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)</span><br><span class=\"line\">    jieba.load_userdict(userDict_path)</span><br><span class=\"line\"></span><br><span class=\"line\">    # 停用词文本</span><br><span class=\"line\">    stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    def get_stopwords_list():</span><br><span class=\"line\">        &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot;</span><br><span class=\"line\">        stopwords_list = [i.strip()</span><br><span class=\"line\">                          for i in codecs.open(stopwords_path).readlines()]</span><br><span class=\"line\">        return stopwords_list</span><br><span class=\"line\"></span><br><span class=\"line\">    # 所有的停用词列表</span><br><span class=\"line\">    stopwords_list = get_stopwords_list()</span><br><span class=\"line\"></span><br><span class=\"line\">    # 分词</span><br><span class=\"line\">    def cut_sentence(sentence):</span><br><span class=\"line\">        &quot;&quot;&quot;对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词&quot;&quot;&quot;</span><br><span class=\"line\">        # print(sentence,&quot;*&quot;*100)</span><br><span class=\"line\">        # eg:[pair(&#x27;今天&#x27;, &#x27;t&#x27;), pair(&#x27;有&#x27;, &#x27;d&#x27;), pair(&#x27;雾&#x27;, &#x27;n&#x27;), pair(&#x27;霾&#x27;, &#x27;g&#x27;)]</span><br><span class=\"line\">        seg_list = pseg.lcut(sentence)</span><br><span class=\"line\">        seg_list = [i for i in seg_list if i.flag not in stopwords_list]</span><br><span class=\"line\">        filtered_words_list = []</span><br><span class=\"line\">        for seg in seg_list:</span><br><span class=\"line\">            # print(seg)</span><br><span class=\"line\">            if len(seg.word) &lt;= 1:</span><br><span class=\"line\">                continue</span><br><span class=\"line\">            elif seg.flag == &quot;eng&quot;:</span><br><span class=\"line\">                if len(seg.word) &lt;= 2:</span><br><span class=\"line\">                    continue</span><br><span class=\"line\">                else:</span><br><span class=\"line\">                    filtered_words_list.append(seg.word)</span><br><span class=\"line\">            elif seg.flag.startswith(&quot;n&quot;):</span><br><span class=\"line\">                filtered_words_list.append(seg.word)</span><br><span class=\"line\">            elif seg.flag in [&quot;x&quot;, &quot;eng&quot;]:  # 是自定一个词语或者是英文单词</span><br><span class=\"line\">                filtered_words_list.append(seg.word)</span><br><span class=\"line\">        return filtered_words_list</span><br><span class=\"line\"></span><br><span class=\"line\">    for row in partition:</span><br><span class=\"line\">        sentence = re.sub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, row.sentence)    # 替换掉标签数据</span><br><span class=\"line\">        words = cut_sentence(sentence)</span><br><span class=\"line\">        yield row.article_id, row.channel_id, words</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>训练模型，得到每个文章词的频率 Counts 结果</p>\n<h1 id=\"词语与词频统计\"><a class=\"markdownIt-Anchor\" href=\"#词语与词频统计\">#</a> 词语与词频统计</h1>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 词语与词频统计</span><br><span class=\"line\">from pyspark.ml.feature import CountVectorizer</span><br><span class=\"line\"># 总词汇的大小，文本中必须出现的次数</span><br><span class=\"line\">cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;countFeatures&quot;, vocabSize=200*10000, minDF=1.0)</span><br><span class=\"line\"># 训练词频统计模型</span><br><span class=\"line\">cv_model = cv.fit(words_df)</span><br><span class=\"line\">cv_model.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)</span><br></pre></td></tr></table></figure>",
            "tags": [
                "推荐",
                "大数据",
                "hadoop"
            ]
        }
    ]
}