{
    "version": "https://jsonfeed.org/version/1",
    "title": "Durtime • All posts by \"一梦文章\" category",
    "description": "The aroma of black tea no longer exists",
    "home_page_url": "https://durtime.github.io/blog",
    "items": [
        {
            "id": "https://durtime.github.io/blog/posts/65ad05d8/",
            "url": "https://durtime.github.io/blog/posts/65ad05d8/",
            "title": "一梦文章推荐业务(一)",
            "date_published": "2022-05-28T14:36:02.000Z",
            "content_html": "<h1 id=\"1-数据库迁移需求\"><a class=\"markdownIt-Anchor\" href=\"#1-数据库迁移需求\">#</a> 1 数据库迁移需求</h1>\n<p>业务 mysql 数据库中的数据，会同步到我们的 hadoop 的 hive 数据仓库中。</p>\n<ul>\n<li>为了避免直接连接、操作业务数据</li>\n<li>同步一份数据在集群中方便进行数据分析操作</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; show databases;</span><br><span class=\"line\">OK</span><br><span class=\"line\">default</span><br><span class=\"line\">profile</span><br><span class=\"line\">toutiao</span><br><span class=\"line\">Time taken: 0.017 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure>\n<p>创建 hive 业务数据库 onedream</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create database if not exists onedream comment &quot;user,news information of onedream mysql&quot; location &#x27;/user/hive/warehouse/onedream.db/&#x27;;</span><br></pre></td></tr></table></figure>\n<h1 id=\"2-sqoop导入\"><a class=\"markdownIt-Anchor\" href=\"#2-sqoop导入\">#</a> 2 sqoop 导入</h1>\n<p>用户：基本信息，关注，收藏，搜索，订阅（设置选择喜好频道）<br>\n文章：分类，文章</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">array=(user_profile user_basic news_user_channel news_channel user_follow user_blacklist user_search news_collection news_article_basic news_article_content news_read news_article_statistic user_material)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">for table_name in $&#123;array[@]&#125;;</span><br><span class=\"line\">do</span><br><span class=\"line\">    sqoop import \\</span><br><span class=\"line\">        --connect jdbc:mysql://192.168.19.137/onedream \\</span><br><span class=\"line\">        --username root \\</span><br><span class=\"line\">        --password password \\</span><br><span class=\"line\">        --table $table_name \\</span><br><span class=\"line\">        --m 5 \\</span><br><span class=\"line\">        --hive-home /root/bigdata/hive \\</span><br><span class=\"line\">        --hive-import \\</span><br><span class=\"line\">        --create-hive-table  \\</span><br><span class=\"line\">        --hive-drop-import-delims \\</span><br><span class=\"line\">        --warehouse-dir /user/hive/warehouse/onedream.db \\</span><br><span class=\"line\">        --hive-table onedream.$table_name</span><br><span class=\"line\">done</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Mysql 导入对应 hive 类型:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL(bigint) --&gt; Hive(bigint) </span><br><span class=\"line\">MySQL(tinyint) --&gt; Hive(tinyint) </span><br><span class=\"line\">MySQL(int) --&gt; Hive(int) </span><br><span class=\"line\">MySQL(double) --&gt; Hive(double) </span><br><span class=\"line\">MySQL(bit) --&gt; Hive(boolean) </span><br><span class=\"line\">MySQL(varchar) --&gt; Hive(string) </span><br><span class=\"line\">MySQL(decimal) --&gt; Hive(double) </span><br><span class=\"line\">MySQL(date/timestamp) --&gt; Hive(string)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"3-tfidf模型的训练\"><a class=\"markdownIt-Anchor\" href=\"#3-tfidf模型的训练\">#</a> 3 TFIDF 模型的训练</h1>\n<p>步骤：<br>\n1、读取 N 篇文章数据<br>\n 2、文章数据进行分词处理<br>\n 3、TFIDF 模型训练保存，spark 使用 count 与 idf 进行计算<br>\n 4、利用模型计算 N 篇文章数据的 TFIDF 值</p>\n<h2 id=\"分词\"><a class=\"markdownIt-Anchor\" href=\"#分词\">#</a> 分词</h2>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 分词</span><br><span class=\"line\">def segmentation(partition):</span><br><span class=\"line\">    import os</span><br><span class=\"line\">    import re</span><br><span class=\"line\"></span><br><span class=\"line\">    import jieba</span><br><span class=\"line\">    import jieba.analyse</span><br><span class=\"line\">    import jieba.posseg as pseg</span><br><span class=\"line\">    import codecs</span><br><span class=\"line\"></span><br><span class=\"line\">    abspath = &quot;/root/words&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    # 结巴加载用户词典</span><br><span class=\"line\">    userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)</span><br><span class=\"line\">    jieba.load_userdict(userDict_path)</span><br><span class=\"line\"></span><br><span class=\"line\">    # 停用词文本</span><br><span class=\"line\">    stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    def get_stopwords_list():</span><br><span class=\"line\">        &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot;</span><br><span class=\"line\">        stopwords_list = [i.strip()</span><br><span class=\"line\">                          for i in codecs.open(stopwords_path).readlines()]</span><br><span class=\"line\">        return stopwords_list</span><br><span class=\"line\"></span><br><span class=\"line\">    # 所有的停用词列表</span><br><span class=\"line\">    stopwords_list = get_stopwords_list()</span><br><span class=\"line\"></span><br><span class=\"line\">    # 分词</span><br><span class=\"line\">    def cut_sentence(sentence):</span><br><span class=\"line\">        &quot;&quot;&quot;对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词&quot;&quot;&quot;</span><br><span class=\"line\">        # print(sentence,&quot;*&quot;*100)</span><br><span class=\"line\">        # eg:[pair(&#x27;今天&#x27;, &#x27;t&#x27;), pair(&#x27;有&#x27;, &#x27;d&#x27;), pair(&#x27;雾&#x27;, &#x27;n&#x27;), pair(&#x27;霾&#x27;, &#x27;g&#x27;)]</span><br><span class=\"line\">        seg_list = pseg.lcut(sentence)</span><br><span class=\"line\">        seg_list = [i for i in seg_list if i.flag not in stopwords_list]</span><br><span class=\"line\">        filtered_words_list = []</span><br><span class=\"line\">        for seg in seg_list:</span><br><span class=\"line\">            # print(seg)</span><br><span class=\"line\">            if len(seg.word) &lt;= 1:</span><br><span class=\"line\">                continue</span><br><span class=\"line\">            elif seg.flag == &quot;eng&quot;:</span><br><span class=\"line\">                if len(seg.word) &lt;= 2:</span><br><span class=\"line\">                    continue</span><br><span class=\"line\">                else:</span><br><span class=\"line\">                    filtered_words_list.append(seg.word)</span><br><span class=\"line\">            elif seg.flag.startswith(&quot;n&quot;):</span><br><span class=\"line\">                filtered_words_list.append(seg.word)</span><br><span class=\"line\">            elif seg.flag in [&quot;x&quot;, &quot;eng&quot;]:  # 是自定一个词语或者是英文单词</span><br><span class=\"line\">                filtered_words_list.append(seg.word)</span><br><span class=\"line\">        return filtered_words_list</span><br><span class=\"line\"></span><br><span class=\"line\">    for row in partition:</span><br><span class=\"line\">        sentence = re.sub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, row.sentence)    # 替换掉标签数据</span><br><span class=\"line\">        words = cut_sentence(sentence)</span><br><span class=\"line\">        yield row.article_id, row.channel_id, words</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>训练模型，得到每个文章词的频率 Counts 结果</p>\n<h1 id=\"词语与词频统计\"><a class=\"markdownIt-Anchor\" href=\"#词语与词频统计\">#</a> 词语与词频统计</h1>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 词语与词频统计</span><br><span class=\"line\">from pyspark.ml.feature import CountVectorizer</span><br><span class=\"line\"># 总词汇的大小，文本中必须出现的次数</span><br><span class=\"line\">cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;countFeatures&quot;, vocabSize=200*10000, minDF=1.0)</span><br><span class=\"line\"># 训练词频统计模型</span><br><span class=\"line\">cv_model = cv.fit(words_df)</span><br><span class=\"line\">cv_model.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)</span><br></pre></td></tr></table></figure>",
            "tags": [
                "推荐",
                "大数据",
                "hadoop"
            ]
        }
    ]
}